**Multimodal VQA (Visual Question Answering) AI Agent**
---
# [TR]

## Proje Amacı
Bu projede, metin tabanlı çalışan **Llama 3.1** modeline görsel algı yeteneği kazandırmak için, **BLIP** ve **CLIP** modellerini sisteme entegre ettim.

##  Ekran Görüntüleri 
<p align="center">
   <img width="2557" height="1099" alt="a" src="https://github.com/user-attachments/assets/e453896f-8618-477e-bd57-9d1af88cd528" />
</p>

## Mimari
1.  **BLIP**
    * Resme bakar ve ne gördüğünü yazıya döker.
    * *Rolü:* Sistemin "Gözü".
2.  **CLIP**
    * BLIP'in gördüğü şey ile resim arasındaki anlamsal bağı kontrol eder.
    * Güven skoru üretir.
    * *Rolü:* Sistemin "Kontrolcüsü".
3.  **LLaMA 3.1** 
    * BLIP'in ürettiği görsel betimlemeyi ve kullanıcı sorusunu sentezleyerek cevap üretir.
    * *Rolü:* Sistemin "Beyni".

---
##  Kullanılan Teknolojiler ve Modeller

| Bileşen | Teknoloji / Model | Link|
| :--- | :--- | :--- |
| **Backend** | FastAPI | |
| **Frontend** | Streamlit | |
| **LLM** | **Llama 3.1 (8B)** | [Llama](https://ollama.com/library/llama3.1) |
| **BLIP** | `Salesforce/blip-image-captioning-large` | [Hugging Face](https://huggingface.co/Salesforce/blip-image-captioning-large) |
| **CLIP** | `openai/clip-vit-base-patch32` | [Hugging Face](https://huggingface.co/openai/clip-vit-base-patch32) |

---

##  Kurulum 

Projeyi yerel makinenizde çalıştırmak için aşağıdaki adımları izleyin.

### 1. Gereksinimler
* Python 3.10 veya üzeri
* [Ollama](https://ollama.com/) (Llama modelini çalıştırmak için)
* Gerekli kütüphaneleri yükleyin.
```bash
  pip install -r requirements.txt
   ```
### 2. env Dosyası
* OLLAMA_URL = Python kodunun, yerel bilgisayarınızda çalışan Ollama ile konuşmasını sağlar.
* LOCAL_URL = Streamlit arayüzünün, resim ve soruları gönderdiği FastAPI sunucu adresidir.
* LLAMA_MODEL = Llama modelinizin ismini yazın.
* CLIP_MODEL = CLIP modelinizin ismini yazın.
* BLIP_MODEL = BLIP modelinizin ismini yazın.

### 3. Çalıştırma
İki ayrı terminal açıp sırasıyla terminallere aşağıdakileri yazın.
```bash
  uvicorn app.main:app --reload
   ```
```bash
  streamlit run ui.py
   ```

## ⚠️ Uyarı
* Bu proje hiçbir şekilde ticari amaç içermemektedir.
* Bu yapay zeka asistanı tarafından üretilen cevaplar hatalı, eksik veya yanıltıcı olabilir. 
* Bu sistem tıbbi teşhis, finansal yatırım veya hukuki danışmanlık gibi kritik kararlarda **kullanılmamalıdır**.
* Modelin ürettiği çıktılar mutlaka bir insan tarafından doğrulanmalıdır.
* Geliştirici, modelin ürettiği yanlış bilgilerden doğacak sonuçlardan sorumlu değildir.

# [EN]
## Project Purpose
In this project, I integrated BLIP and CLIP models into the system to provide visual perception capabilities to the text-based Llama 3.1 model.

##  Screenshots 
<p align="center">
   <img width="2557" height="1099" alt="a" src="https://github.com/user-attachments/assets/e453896f-8618-477e-bd57-9d1af88cd528" />
</p>

## Architecture
1. **BLIP**
   * It looks at the image and transcribes what it sees into text.
   * Role: The "Eye" of the system.

2. **CLIP**
   * It checks the semantic connection between what BLIP sees and the image.
   * It generates a confidence score.
   * Role: The "Controller" of the system.

3. **LLaMA 3.1**
   * It generates an answer by synthesizing the visual description produced by BLIP and the user's question.
   * Role: The "Brain" of the system.

##  Technologies and Models Used

| Component | Technology / Model | Link |
| :--- | :--- | :--- |
| **Backend** | FastAPI | |
| **Frontend** | Streamlit | |
| **LLM** | **Llama 3.1 (8B)** | [Llama](https://ollama.com/library/llama3.1) |
| **BLIP** | `Salesforce/blip-image-captioning-large` | [Hugging Face](https://huggingface.co/Salesforce/blip-image-captioning-large) |
| **CLIP** | `openai/clip-vit-base-patch32` | [Hugging Face](https://huggingface.co/openai/clip-vit-base-patch32) |

---

## Setup
Follow the steps below to run the project on your local machine.

### 1. Requirements
* Python 3.10 or higher
* [Ollama](https://ollama.com/) (Running the Llama Model)
* Install the required libraries.
```bash
  pip install -r requirements.txt
   ```
### 2. env File
* OLLAMA_URL = Allows the Python code to communicate with Ollama running on your local computer.
* LOCAL_URL = The FastAPI server address where the Streamlit interface sends images and questions.
* LLAMA_MODEL = Write the name of your Llama model.
* CLIP_MODEL = Write the name of your CLIP model.
* BLIP_MODEL = Write the name of your BLIP model.

### 3. Execution
Open two separate terminals and type the following into the terminals respectively.
```bash
  uvicorn app.main:app --reload
   ```
```bash
  streamlit run ui.py
   ```

## ⚠️ Warning
* This project does not contain any commercial purpose.
* The responses generated by this AI assistant may be inaccurate, incomplete, or misleading.
* This system **must not be used** for critical decisions such as medical diagnosis, financial investment, or legal advice.
* The outputs generated by the model should always be verified by a human.
* The developer is not liable for any consequences arising from incorrect information produced by the model.
